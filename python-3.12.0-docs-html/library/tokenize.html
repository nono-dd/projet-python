
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="tokenize — Analyseur lexical de Python" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://docs.python.org/3/library/tokenize.html" />
<meta property="og:site_name" content="Python documentation" />
<meta property="og:description" content="Code source : Lib/tokenize.py The tokenize module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it ..." />
<meta property="og:image" content="https://docs.python.org/3/_static/og-image.png" />
<meta property="og:image:alt" content="Python documentation" />
<meta name="description" content="Code source : Lib/tokenize.py The tokenize module provides a lexical scanner for Python source code, implemented in Python. The scanner in this module returns comments as tokens as well, making it ..." />
<meta property="og:image:width" content="200" />
<meta property="og:image:height" content="200" />
<meta name="theme-color" content="#3776ab" />

    <title>tokenize — Analyseur lexical de Python &#8212; Documentation Python 3.12.0</title><meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/pydoctheme.css?digest=b37c26da2f7529d09fe70b41c4b2133fe4931a90" />
    <link id="pygments_dark_css" media="(prefers-color-scheme: dark)" rel="stylesheet" type="text/css" href="../_static/pygments_dark.css" />
    
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/translations.js"></script>
    
    <script src="../_static/sidebar.js"></script>
    
    <link rel="search" type="application/opensearchdescription+xml"
          title="Recherchez dans Documentation Python 3.12.0"
          href="../_static/opensearch.xml"/>
    <link rel="author" title="À propos de ces documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Recherche" href="../search.html" />
    <link rel="copyright" title="Copyright" href="../copyright.html" />
    <link rel="next" title="tabnanny — Détection d&#39;indentation ambiguë" href="tabnanny.html" />
    <link rel="prev" title="keyword — Tester si des chaînes sont des mot-clés Python" href="keyword.html" />
    <link rel="canonical" href="https://docs.python.org/3/library/tokenize.html" />
    
      
    

    
    <style>
      @media only screen {
        table.full-width-table {
            width: 100%;
        }
      }
    </style>
<link rel="stylesheet" href="../_static/pydoctheme_dark.css" media="(prefers-color-scheme: dark)" id="pydoctheme_dark_css">
    <link rel="shortcut icon" type="image/png" href="../_static/py.svg" />
            <script type="text/javascript" src="../_static/copybutton.js"></script>
            <script type="text/javascript" src="../_static/menu.js"></script>
            <script type="text/javascript" src="../_static/search-focus.js"></script>
            <script type="text/javascript" src="../_static/themetoggle.js"></script> 

  </head>
<body>
<div class="mobile-nav">
    <input type="checkbox" id="menuToggler" class="toggler__input" aria-controls="navigation"
           aria-pressed="false" aria-expanded="false" role="button" aria-label="Menu" />
    <nav class="nav-content" role="navigation">
        <label for="menuToggler" class="toggler__label">
            <span></span>
        </label>
        <span class="nav-items-wrapper">
            <a href="https://www.python.org/" class="nav-logo">
                <img src="../_static/py.svg" alt="Logo"/>
            </a>
            <span class="version_switcher_placeholder"></span>
            <form role="search" class="search" action="../search.html" method="get">
                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" class="search-icon">
                    <path fill-rule="nonzero" fill="currentColor" d="M15.5 14h-.79l-.28-.27a6.5 6.5 0 001.48-5.34c-.47-2.78-2.79-5-5.59-5.34a6.505 6.505 0 00-7.27 7.27c.34 2.8 2.56 5.12 5.34 5.59a6.5 6.5 0 005.34-1.48l.27.28v.79l4.25 4.25c.41.41 1.08.41 1.49 0 .41-.41.41-1.08 0-1.49L15.5 14zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path>
                </svg>
                <input placeholder="Recherche rapide" aria-label="Recherche rapide" type="search" name="q" />
                <input type="submit" value="Go"/>
            </form>
        </span>
    </nav>
    <div class="menu-wrapper">
        <nav class="menu" role="navigation" aria-label="main navigation">
            <div class="language_switcher_placeholder"></div>
            
<label class="theme-selector-label">
    Theme
    <select class="theme-selector" oninput="activateTheme(this.value)">
        <option value="auto" selected>Auto</option>
        <option value="light">Light</option>
        <option value="dark">Dark</option>
    </select>
</label>
  <div>
    <h3><a href="../contents.html">Table des matières</a></h3>
    <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code> — Analyseur lexical de Python</a><ul>
<li><a class="reference internal" href="#tokenizing-input">Analyse Lexicale</a></li>
<li><a class="reference internal" href="#command-line-usage">Utilisation en ligne de commande.</a></li>
<li><a class="reference internal" href="#examples">Exemples</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Sujet précédent</h4>
    <p class="topless"><a href="keyword.html"
                          title="Chapitre précédent"><code class="xref py py-mod docutils literal notranslate"><span class="pre">keyword</span></code> — Tester si des chaînes sont des mot-clés Python</a></p>
  </div>
  <div>
    <h4>Sujet suivant</h4>
    <p class="topless"><a href="tabnanny.html"
                          title="Chapitre suivant"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tabnanny</span></code> — Détection d'indentation ambiguë</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>Cette page</h3>
    <ul class="this-page-menu">
      <li><a href="../bugs.html">Signalement de bogue</a></li>
      <li>
        <a href="https://github.com/python/cpython/blob/main/Doc/library/tokenize.rst"
            rel="nofollow">Voir la source
        </a>
      </li>
    </ul>
  </div>
        </nav>
    </div>
</div>

  
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="Index général"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Index des modules Python"
             >modules</a> |</li>
        <li class="right" >
          <a href="tabnanny.html" title="tabnanny — Détection d&#39;indentation ambiguë"
             accesskey="N">suivant</a> |</li>
        <li class="right" >
          <a href="keyword.html" title="keyword — Tester si des chaînes sont des mot-clés Python"
             accesskey="P">précédent</a> |</li>

          <li><img src="../_static/py.svg" alt="python logo" style="vertical-align: middle; margin-top: -1px"/></li>
          <li><a href="https://www.python.org/">Python</a> &#187;</li>
          <li class="switchers">
            <div class="language_switcher_placeholder"></div>
            <div class="version_switcher_placeholder"></div>
          </li>
          <li>
              
          </li>
    <li id="cpython-language-and-version">
      <a href="../index.html">3.12.0 Documentation</a> &#187;
    </li>

          <li class="nav-item nav-item-1"><a href="index.html" >La bibliothèque standard</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="language.html" accesskey="U">Services du Langage Python</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code> — Analyseur lexical de Python</a></li>
                <li class="right">
                    

    <div class="inline-search" role="search">
        <form class="inline-search" action="../search.html" method="get">
          <input placeholder="Recherche rapide" aria-label="Recherche rapide" type="search" name="q" id="search-box" />
          <input type="submit" value="Go" />
        </form>
    </div>
                     |
                </li>
            <li class="right">
<label class="theme-selector-label">
    Theme
    <select class="theme-selector" oninput="activateTheme(this.value)">
        <option value="auto" selected>Auto</option>
        <option value="light">Light</option>
        <option value="dark">Dark</option>
    </select>
</label> |</li>
            
      </ul>
    </div>    

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="module-tokenize">
<span id="tokenize-tokenizer-for-python-source"></span><h1><a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code></a> — Analyseur lexical de Python<a class="headerlink" href="#module-tokenize" title="Lien permanent vers ce titre">¶</a></h1>
<p><strong>Code source :</strong> <a class="reference external" href="https://github.com/python/cpython/tree/3.12/Lib/tokenize.py">Lib/tokenize.py</a></p>
<hr class="docutils" />
<p>The <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code></a> module provides a lexical scanner for Python source code,
implemented in Python.  The scanner in this module returns comments as tokens
as well, making it useful for implementing &quot;pretty-printers&quot;, including
colorizers for on-screen displays.</p>
<p>Pour simplifier la gestion de flux de <em>tokens</em>, tous les <em>tokens</em> <a class="reference internal" href="../reference/lexical_analysis.html#operators"><span class="std std-ref">operator</span></a> et <a class="reference internal" href="../reference/lexical_analysis.html#delimiters"><span class="std std-ref">delimiter</span></a>, ainsi que les <code class="xref py py-data docutils literal notranslate"><span class="pre">Ellipsis*</span></code> sont renvoyés en utilisant le <em>token</em> générique <a class="reference internal" href="token.html#token.OP" title="token.OP"><code class="xref py py-data docutils literal notranslate"><span class="pre">OP</span></code></a>. Le type exact peut être déterminé en vérifiant la propriété <code class="docutils literal notranslate"><span class="pre">exact_type</span></code> du <a class="reference internal" href="../glossary.html#term-named-tuple"><span class="xref std std-term">named tuple</span></a> renvoyé par <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize.tokenize()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Avertissement</p>
<p>Note that the functions in this module are only designed to parse
syntactically valid Python code (code that does not raise when parsed
using <a class="reference internal" href="ast.html#ast.parse" title="ast.parse"><code class="xref py py-func docutils literal notranslate"><span class="pre">ast.parse()</span></code></a>).  The behavior of the functions in this module is
<strong>undefined</strong> when providing invalid Python code and it can change at any
point.</p>
</div>
<section id="tokenizing-input">
<h2>Analyse Lexicale<a class="headerlink" href="#tokenizing-input" title="Lien permanent vers ce titre">¶</a></h2>
<p>Le point d'entrée principal est un <a class="reference internal" href="../glossary.html#term-generator"><span class="xref std std-term">générateur</span></a> :</p>
<dl class="py function">
<dt class="sig sig-object py" id="tokenize.tokenize">
<span class="sig-prename descclassname"><span class="pre">tokenize.</span></span><span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">readline</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tokenize.tokenize" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Le générateur <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a> prend un argument <em>readline</em> qui doit être un objet appelable exposant la même interface que la méthode <a class="reference internal" href="io.html#io.IOBase.readline" title="io.IOBase.readline"><code class="xref py py-meth docutils literal notranslate"><span class="pre">io.IOBase.readline()</span></code></a> des objets fichiers. Chaque appel a la fonction doit renvoyer une ligne sous forme de <em>bytes</em>.</p>
<p>The generator produces 5-tuples with these members: the token type; the
token string; a 2-tuple <code class="docutils literal notranslate"><span class="pre">(srow,</span> <span class="pre">scol)</span></code> of ints specifying the row and
column where the token begins in the source; a 2-tuple <code class="docutils literal notranslate"><span class="pre">(erow,</span> <span class="pre">ecol)</span></code> of
ints specifying the row and column where the token ends in the source; and
the line on which the token was found. The line passed (the last tuple item)
is the <em>physical</em> line.  The 5 tuple is returned as a <a class="reference internal" href="../glossary.html#term-named-tuple"><span class="xref std std-term">named tuple</span></a>
with the field names:
<code class="docutils literal notranslate"><span class="pre">type</span> <span class="pre">string</span> <span class="pre">start</span> <span class="pre">end</span> <span class="pre">line</span></code>.</p>
<p>Le <a class="reference internal" href="../glossary.html#term-named-tuple"><span class="xref std std-term">n-uplet nommé</span></a> a une propriété additionnelle appelée <code class="docutils literal notranslate"><span class="pre">exact_type</span></code> qui contient le type exact de l'opérateur pour les jetons <a class="reference internal" href="token.html#token.OP" title="token.OP"><code class="xref py py-data docutils literal notranslate"><span class="pre">OP</span></code></a>. Pour tous les autres types de jetons, <code class="docutils literal notranslate"><span class="pre">exact_type</span></code> est égal au champ <code class="docutils literal notranslate"><span class="pre">type</span></code> du <em>n</em>-uplet nommé.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Modifié dans la version 3.1: </span>prise en charge des <em>n</em>-uplets nommés.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Modifié dans la version 3.3: </span>prise en charge de <code class="docutils literal notranslate"><span class="pre">exact_type</span></code>.</p>
</div>
<p><a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a> détermine le codage source du fichier en recherchant une nomenclature UTF-8 ou un cookie d'encodage, selon la <span class="target" id="index-4"></span><a class="pep reference external" href="https://peps.python.org/pep-0263/"><strong>PEP 263</strong></a>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tokenize.generate_tokens">
<span class="sig-prename descclassname"><span class="pre">tokenize.</span></span><span class="sig-name descname"><span class="pre">generate_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">readline</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tokenize.generate_tokens" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Tokenize a source reading unicode strings instead of bytes.</p>
<p>Like <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a>, the <em>readline</em> argument is a callable returning
a single line of input. However, <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate_tokens()</span></code></a> expects <em>readline</em>
to return a str object rather than bytes.</p>
<p>The result is an iterator yielding named tuples, exactly like
<a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a>. It does not yield an <a class="reference internal" href="token.html#token.ENCODING" title="token.ENCODING"><code class="xref py py-data docutils literal notranslate"><span class="pre">ENCODING</span></code></a> token.</p>
</dd></dl>

<p>Toutes les constantes du module <a class="reference internal" href="token.html#module-token" title="token: Constants representing terminal nodes of the parse tree."><code class="xref py py-mod docutils literal notranslate"><span class="pre">token</span></code></a> sont également exportées depuis module <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code></a>.</p>
<p>Une autre fonction est fournie pour inverser le processus de tokenisation. Ceci est utile pour créer des outils permettant de codifier un script, de modifier le flux de jetons et de réécrire le script modifié.</p>
<dl class="py function">
<dt class="sig sig-object py" id="tokenize.untokenize">
<span class="sig-prename descclassname"><span class="pre">tokenize.</span></span><span class="sig-name descname"><span class="pre">untokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iterable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tokenize.untokenize" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Convertit les jetons en code source Python. L'<em>iterable</em> doit renvoyer des séquences avec au moins deux éléments, le type de jeton et la chaîne de caractères associée. Tout élément de séquence supplémentaire est ignoré.</p>
<p>Le script reconstruit est renvoyé sous la forme d'une chaîne unique. Le résultat est garanti pour que le jeton corresponde à l'entrée afin que la conversion soit sans perte et que les allers et retours soient assurés.  La garantie ne s'applique qu'au type de jeton et à la chaîne de jetons car l'espacement entre les jetons (positions des colonnes) peut changer.</p>
<p>It returns bytes, encoded using the <a class="reference internal" href="token.html#token.ENCODING" title="token.ENCODING"><code class="xref py py-data docutils literal notranslate"><span class="pre">ENCODING</span></code></a> token, which
is the first token sequence output by <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a>. If there is no
encoding token in the input, it returns a str instead.</p>
</dd></dl>

<p><a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a> a besoin de détecter le codage des fichiers sources qu'il code. La fonction utilisée pour cela est disponible :</p>
<dl class="py function">
<dt class="sig sig-object py" id="tokenize.detect_encoding">
<span class="sig-prename descclassname"><span class="pre">tokenize.</span></span><span class="sig-name descname"><span class="pre">detect_encoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">readline</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tokenize.detect_encoding" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>La fonction <a class="reference internal" href="#tokenize.detect_encoding" title="tokenize.detect_encoding"><code class="xref py py-func docutils literal notranslate"><span class="pre">detect_encoding()</span></code></a> est utilisée pour détecter l'encodage à utiliser pour décoder un fichier source Python. Il nécessite un seul argument, <em>readline</em>, de la même manière que le générateur <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a>.</p>
<p>Il appelle <em>readline</em> au maximum deux fois et renvoie le codage utilisé (sous forme de chaîne) et une liste de toutes les lignes (non décodées à partir des octets) dans lesquelles il a été lu.</p>
<p>Il détecte l'encodage par la présence d'une marqueur UTF-8 (<em>BOM</em>) ou d'un cookie de codage, comme spécifié dans la <span class="target" id="index-5"></span><a class="pep reference external" href="https://peps.python.org/pep-0263/"><strong>PEP 263</strong></a>. Si un <em>BOM</em> et un cookie sont présents, mais en désaccord, un <a class="reference internal" href="exceptions.html#SyntaxError" title="SyntaxError"><code class="xref py py-exc docutils literal notranslate"><span class="pre">SyntaxError</span></code></a> sera levée. Notez que si le <em>BOM</em> est trouvé, <code class="docutils literal notranslate"><span class="pre">'utf-8-sig'</span></code> sera renvoyé comme encodage.</p>
<p>Si aucun codage n'est spécifié, la valeur par défaut, <code class="docutils literal notranslate"><span class="pre">'utf-8'</span></code>, sera renvoyée.</p>
<p>Utilisez <a class="reference internal" href="#tokenize.open" title="tokenize.open"><code class="xref py py-func docutils literal notranslate"><span class="pre">open()</span></code></a> pour ouvrir les fichiers source Python : ça utilise <a class="reference internal" href="#tokenize.detect_encoding" title="tokenize.detect_encoding"><code class="xref py py-func docutils literal notranslate"><span class="pre">detect_encoding()</span></code></a> pour détecter le codage du fichier.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tokenize.open">
<span class="sig-prename descclassname"><span class="pre">tokenize.</span></span><span class="sig-name descname"><span class="pre">open</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tokenize.open" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Ouvre un fichier en mode lecture seule en utilisant l'encodage détecté par <code class="xref py py-func docutils literal notranslate"><span class="pre">dectect_encoding()</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">Nouveau dans la version 3.2.</span></p>
</div>
</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="tokenize.TokenError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tokenize.</span></span><span class="sig-name descname"><span class="pre">TokenError</span></span><a class="headerlink" href="#tokenize.TokenError" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Déclenché lorsque soit une <em>docstring</em> soit une expression qui pourrait être divisée sur plusieurs lignes n'est pas complété dans le fichier, par exemple :</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;&quot;&quot;Beginning of</span>
<span class="s2">docstring</span>
</pre></div>
</div>
<p>ou :</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">1</span><span class="p">,</span>
 <span class="mi">2</span><span class="p">,</span>
 <span class="mi">3</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="command-line-usage">
<span id="tokenize-cli"></span><h2>Utilisation en ligne de commande.<a class="headerlink" href="#command-line-usage" title="Lien permanent vers ce titre">¶</a></h2>
<div class="versionadded">
<p><span class="versionmodified added">Nouveau dans la version 3.3.</span></p>
</div>
<p>Le module <a class="reference internal" href="#module-tokenize" title="tokenize: Lexical scanner for Python source code."><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code></a> peut être exécuté en tant que script à partir de la ligne de commande. C'est aussi simple que :</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>tokenize<span class="w"> </span><span class="o">[</span>-e<span class="o">]</span><span class="w"> </span><span class="o">[</span>filename.py<span class="o">]</span>
</pre></div>
</div>
<p>Les options suivantes sont acceptées :</p>
<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-tokenize-h">
<span id="cmdoption-tokenize-help"></span><span class="sig-name descname"><span class="pre">-h</span></span><span class="sig-prename descclassname"></span><span class="sig-prename descclassname"><span class="pre">,</span> </span><span class="sig-name descname"><span class="pre">--help</span></span><span class="sig-prename descclassname"></span><a class="headerlink" href="#cmdoption-tokenize-h" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Montre ce message d'aide et quitte</p>
</dd></dl>

<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-tokenize-e">
<span id="cmdoption-tokenize-exact"></span><span class="sig-name descname"><span class="pre">-e</span></span><span class="sig-prename descclassname"></span><span class="sig-prename descclassname"><span class="pre">,</span> </span><span class="sig-name descname"><span class="pre">--exact</span></span><span class="sig-prename descclassname"></span><a class="headerlink" href="#cmdoption-tokenize-e" title="Lien permanent vers cette définition">¶</a></dt>
<dd><p>Affiche les noms de jetons en utilisant le même type.</p>
</dd></dl>

<p>Si <code class="file docutils literal notranslate"><span class="pre">filename.py</span></code> est spécifié, son contenu est tokenisé vers <em>stdout</em>. Sinon, la tokenisation est effectuée sur ce qui est fourni sur <em>stdin</em>.</p>
</section>
<section id="examples">
<h2>Exemples<a class="headerlink" href="#examples" title="Lien permanent vers ce titre">¶</a></h2>
<p>Exemple d'un script qui transforme les littéraux de type <em>float</em> en type <em>Decimal</em> :</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tokenize</span> <span class="kn">import</span> <span class="n">tokenize</span><span class="p">,</span> <span class="n">untokenize</span><span class="p">,</span> <span class="n">NUMBER</span><span class="p">,</span> <span class="n">STRING</span><span class="p">,</span> <span class="n">NAME</span><span class="p">,</span> <span class="n">OP</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>

<span class="k">def</span> <span class="nf">decistmt</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Substitute Decimals for floats in a string of statements.</span>

<span class="sd">    &gt;&gt;&gt; from decimal import Decimal</span>
<span class="sd">    &gt;&gt;&gt; s = &#39;print(+21.3e-5*-.1234/81.7)&#39;</span>
<span class="sd">    &gt;&gt;&gt; decistmt(s)</span>
<span class="sd">    &quot;print (+Decimal (&#39;21.3e-5&#39;)*-Decimal (&#39;.1234&#39;)/Decimal (&#39;81.7&#39;))&quot;</span>

<span class="sd">    The format of the exponent is inherited from the platform C library.</span>
<span class="sd">    Known cases are &quot;e-007&quot; (Windows) and &quot;e-07&quot; (not Windows).  Since</span>
<span class="sd">    we&#39;re only showing 12 digits, and the 13th isn&#39;t close to 5, the</span>
<span class="sd">    rest of the output should be platform-independent.</span>

<span class="sd">    &gt;&gt;&gt; exec(s)  #doctest: +ELLIPSIS</span>
<span class="sd">    -3.21716034272e-0...7</span>

<span class="sd">    Output from calculations with Decimal should be identical across all</span>
<span class="sd">    platforms.</span>

<span class="sd">    &gt;&gt;&gt; exec(decistmt(s))</span>
<span class="sd">    -3.217160342717258261933904529E-7</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">readline</span><span class="p">)</span>  <span class="c1"># tokenize the string</span>
    <span class="k">for</span> <span class="n">toknum</span><span class="p">,</span> <span class="n">tokval</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">g</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">toknum</span> <span class="o">==</span> <span class="n">NUMBER</span> <span class="ow">and</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">tokval</span><span class="p">:</span>  <span class="c1"># replace NUMBER tokens</span>
            <span class="n">result</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
                <span class="p">(</span><span class="n">NAME</span><span class="p">,</span> <span class="s1">&#39;Decimal&#39;</span><span class="p">),</span>
                <span class="p">(</span><span class="n">OP</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">),</span>
                <span class="p">(</span><span class="n">STRING</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">tokval</span><span class="p">)),</span>
                <span class="p">(</span><span class="n">OP</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">)</span>
            <span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">toknum</span><span class="p">,</span> <span class="n">tokval</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">untokenize</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Exemple de tokenisation à partir de la ligne de commande. Le script :</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">say_hello</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hello, World!&quot;</span><span class="p">)</span>

<span class="n">say_hello</span><span class="p">()</span>
</pre></div>
</div>
<p>sera tokenisé à la sortie suivante où la première colonne est la plage des coordonnées de la ligne/colonne où se trouve le jeton, la deuxième colonne est le nom du jeton, et la dernière colonne est la valeur du jeton (le cas échéant)</p>
<div class="highlight-shell-session notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>tokenize<span class="w"> </span>hello.py
<span class="go">0,0-0,0:            ENCODING       &#39;utf-8&#39;</span>
<span class="go">1,0-1,3:            NAME           &#39;def&#39;</span>
<span class="go">1,4-1,13:           NAME           &#39;say_hello&#39;</span>
<span class="go">1,13-1,14:          OP             &#39;(&#39;</span>
<span class="go">1,14-1,15:          OP             &#39;)&#39;</span>
<span class="go">1,15-1,16:          OP             &#39;:&#39;</span>
<span class="go">1,16-1,17:          NEWLINE        &#39;\n&#39;</span>
<span class="go">2,0-2,4:            INDENT         &#39;    &#39;</span>
<span class="go">2,4-2,9:            NAME           &#39;print&#39;</span>
<span class="go">2,9-2,10:           OP             &#39;(&#39;</span>
<span class="go">2,10-2,25:          STRING         &#39;&quot;Hello, World!&quot;&#39;</span>
<span class="go">2,25-2,26:          OP             &#39;)&#39;</span>
<span class="go">2,26-2,27:          NEWLINE        &#39;\n&#39;</span>
<span class="go">3,0-3,1:            NL             &#39;\n&#39;</span>
<span class="go">4,0-4,0:            DEDENT         &#39;&#39;</span>
<span class="go">4,0-4,9:            NAME           &#39;say_hello&#39;</span>
<span class="go">4,9-4,10:           OP             &#39;(&#39;</span>
<span class="go">4,10-4,11:          OP             &#39;)&#39;</span>
<span class="go">4,11-4,12:          NEWLINE        &#39;\n&#39;</span>
<span class="go">5,0-5,0:            ENDMARKER      &#39;&#39;</span>
</pre></div>
</div>
<p>Les noms exacts des types de jeton peuvent être affichés en utilisant l’option : <a class="reference internal" href="#cmdoption-tokenize-e"><code class="xref std std-option docutils literal notranslate"><span class="pre">-e</span></code></a> :</p>
<div class="highlight-shell-session notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>tokenize<span class="w"> </span>-e<span class="w"> </span>hello.py
<span class="go">0,0-0,0:            ENCODING       &#39;utf-8&#39;</span>
<span class="go">1,0-1,3:            NAME           &#39;def&#39;</span>
<span class="go">1,4-1,13:           NAME           &#39;say_hello&#39;</span>
<span class="go">1,13-1,14:          LPAR           &#39;(&#39;</span>
<span class="go">1,14-1,15:          RPAR           &#39;)&#39;</span>
<span class="go">1,15-1,16:          COLON          &#39;:&#39;</span>
<span class="go">1,16-1,17:          NEWLINE        &#39;\n&#39;</span>
<span class="go">2,0-2,4:            INDENT         &#39;    &#39;</span>
<span class="go">2,4-2,9:            NAME           &#39;print&#39;</span>
<span class="go">2,9-2,10:           LPAR           &#39;(&#39;</span>
<span class="go">2,10-2,25:          STRING         &#39;&quot;Hello, World!&quot;&#39;</span>
<span class="go">2,25-2,26:          RPAR           &#39;)&#39;</span>
<span class="go">2,26-2,27:          NEWLINE        &#39;\n&#39;</span>
<span class="go">3,0-3,1:            NL             &#39;\n&#39;</span>
<span class="go">4,0-4,0:            DEDENT         &#39;&#39;</span>
<span class="go">4,0-4,9:            NAME           &#39;say_hello&#39;</span>
<span class="go">4,9-4,10:           LPAR           &#39;(&#39;</span>
<span class="go">4,10-4,11:          RPAR           &#39;)&#39;</span>
<span class="go">4,11-4,12:          NEWLINE        &#39;\n&#39;</span>
<span class="go">5,0-5,0:            ENDMARKER      &#39;&#39;</span>
</pre></div>
</div>
<p>Example of tokenizing a file programmatically, reading unicode
strings instead of bytes with <a class="reference internal" href="#tokenize.generate_tokens" title="tokenize.generate_tokens"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate_tokens()</span></code></a>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tokenize</span>

<span class="k">with</span> <span class="n">tokenize</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;hello.py&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="o">.</span><span class="n">generate_tokens</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
<p>Or reading bytes directly with <a class="reference internal" href="#tokenize.tokenize" title="tokenize.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tokenize</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;hello.py&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../contents.html">Table des matières</a></h3>
    <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code> — Analyseur lexical de Python</a><ul>
<li><a class="reference internal" href="#tokenizing-input">Analyse Lexicale</a></li>
<li><a class="reference internal" href="#command-line-usage">Utilisation en ligne de commande.</a></li>
<li><a class="reference internal" href="#examples">Exemples</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Sujet précédent</h4>
    <p class="topless"><a href="keyword.html"
                          title="Chapitre précédent"><code class="xref py py-mod docutils literal notranslate"><span class="pre">keyword</span></code> — Tester si des chaînes sont des mot-clés Python</a></p>
  </div>
  <div>
    <h4>Sujet suivant</h4>
    <p class="topless"><a href="tabnanny.html"
                          title="Chapitre suivant"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tabnanny</span></code> — Détection d'indentation ambiguë</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>Cette page</h3>
    <ul class="this-page-menu">
      <li><a href="../bugs.html">Signalement de bogue</a></li>
      <li>
        <a href="https://github.com/python/cpython/blob/main/Doc/library/tokenize.rst"
            rel="nofollow">Voir la source
        </a>
      </li>
    </ul>
  </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>  
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="Index général"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Index des modules Python"
             >modules</a> |</li>
        <li class="right" >
          <a href="tabnanny.html" title="tabnanny — Détection d&#39;indentation ambiguë"
             >suivant</a> |</li>
        <li class="right" >
          <a href="keyword.html" title="keyword — Tester si des chaînes sont des mot-clés Python"
             >précédent</a> |</li>

          <li><img src="../_static/py.svg" alt="python logo" style="vertical-align: middle; margin-top: -1px"/></li>
          <li><a href="https://www.python.org/">Python</a> &#187;</li>
          <li class="switchers">
            <div class="language_switcher_placeholder"></div>
            <div class="version_switcher_placeholder"></div>
          </li>
          <li>
              
          </li>
    <li id="cpython-language-and-version">
      <a href="../index.html">3.12.0 Documentation</a> &#187;
    </li>

          <li class="nav-item nav-item-1"><a href="index.html" >La bibliothèque standard</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="language.html" >Services du Langage Python</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><code class="xref py py-mod docutils literal notranslate"><span class="pre">tokenize</span></code> — Analyseur lexical de Python</a></li>
                <li class="right">
                    

    <div class="inline-search" role="search">
        <form class="inline-search" action="../search.html" method="get">
          <input placeholder="Recherche rapide" aria-label="Recherche rapide" type="search" name="q" id="search-box" />
          <input type="submit" value="Go" />
        </form>
    </div>
                     |
                </li>
            <li class="right">
<label class="theme-selector-label">
    Theme
    <select class="theme-selector" oninput="activateTheme(this.value)">
        <option value="auto" selected>Auto</option>
        <option value="light">Light</option>
        <option value="dark">Dark</option>
    </select>
</label> |</li>
            
      </ul>
    </div>  
    <div class="footer">
    &copy; <a href="../copyright.html">Copyright</a> 2001-2023, Python Software Foundation.
    <br />
    This page is licensed under the Python Software Foundation License Version 2.
    <br />
    Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.
    <br />
    See <a href="/license.html">History and License</a> for more information.<br />
    <br />

    The Python Software Foundation is a non-profit corporation.
<a href="https://www.python.org/psf/donations/">Please donate.</a>
<br />
    <br />

    Mis à jour le oct. 26, 2023.
    <a href="/bugs.html">Found a bug</a>?
    <br />

    Créé en utilisant <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    </div>

  </body>
</html>